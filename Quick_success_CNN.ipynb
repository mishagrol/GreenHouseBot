{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47d8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9048d519",
   "metadata": {
    "id": "22f283c5"
   },
   "source": [
    "# **<span style='color:#F1A424'>Innavation Workshop - 2022. Quick success</span>**\n",
    "#### **<span style='color:#F1A424'>Practical part of Quick Success, Skoltech, Moscow-2022</span>**\n",
    "\n",
    "**Instructors:** Elizveta Kiseleva, Mikhail Gasanov, Anna Petrovskaia\n",
    "\n",
    "To prepare this seminar the following resources were used:\n",
    "\n",
    "[kaggle](https://www.kaggle.com/code/shtrausslearning/pytorch-cnn-binary-image-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cba26",
   "metadata": {
    "id": "aa75866c"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torchvision import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92caea6f",
   "metadata": {
    "id": "407b1578"
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3934a",
   "metadata": {
    "id": "cb4965d6"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1dd151",
   "metadata": {
    "id": "d60e6a78"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "- Load the dataset information file; train_labels.csv, it contains a reference to an image ID (id) & its classification allocation (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729555d5",
   "metadata": {
    "id": "88Y4vR-djjst"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062db8d",
   "metadata": {
    "id": "C3C-XeolyGlU"
   },
   "outputs": [],
   "source": [
    "first_class = 'mint'\n",
    "second_class = 'rosemary'\n",
    "\n",
    "first_class_dr = '/content/drive/MyDrive/Workshop/GreenHouseBot_data/input/' + first_class +'/'\n",
    "second_class_dr = '/content/drive/MyDrive/Workshop/GreenHouseBot_data/input/' + second_class + '/'\n",
    "\n",
    "first_class_names = os.listdir(first_class_dr)\n",
    "second_class_names = os.listdir(second_class_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d86faf",
   "metadata": {
    "id": "pWlG4gFy7nPf"
   },
   "outputs": [],
   "source": [
    "def rename_files (path, names):\n",
    "    for i in range(len(names)):\n",
    "        folder_name = path.split('/')[-2] \n",
    "        name, ext = names[i].split('.')\n",
    "        os.rename(path + names[i], path + folder_name + str(i) + '.' + ext)\n",
    "    \n",
    "rename_files(first_class_dr, first_class_names)\n",
    "rename_files(second_class_dr, second_class_names)\n",
    "\n",
    "first_class_names = os.listdir(first_class_dr)\n",
    "second_class_names = os.listdir(second_class_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d773a",
   "metadata": {
    "id": "xY0hfBjrnbXQ"
   },
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(columns=('id','label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8842c",
   "metadata": {
    "id": "drQEk2zDlkFw"
   },
   "outputs": [],
   "source": [
    "labels_df['id'] = first_class_names + second_class_names\n",
    "labels_df['label'] = np.concatenate((np.zeros(int(len(first_class_names)), dtype = int), np.ones(int(len(second_class_names))  , dtype = int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab69324",
   "metadata": {
    "id": "830da472"
   },
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc9a11",
   "metadata": {
    "id": "0bcd6222"
   },
   "outputs": [],
   "source": [
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543d540",
   "metadata": {
    "id": "AGbdF78QhBhK"
   },
   "outputs": [],
   "source": [
    "labels_df.to_csv('/content/drive/MyDrive/Workshop/GreenHouseBot_data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6b144",
   "metadata": {
    "id": "837d43f5"
   },
   "source": [
    "## Check for duplicate entries\n",
    "\n",
    "- Check if the dataset contains any duplicates, if there is we should drop them, which we have none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd10670",
   "metadata": {
    "id": "4d2bcb0e"
   },
   "outputs": [],
   "source": [
    "# No duplicate ids found\n",
    "labels_df[labels_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43669d10",
   "metadata": {
    "id": "8e3f5ce5"
   },
   "source": [
    "## Target feature class balance\n",
    "- Let's check the number of object in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203b37c",
   "metadata": {
    "id": "934bba50"
   },
   "outputs": [],
   "source": [
    "labels_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78059175",
   "metadata": {
    "id": "f5a674fc"
   },
   "source": [
    "## Organizing folders\n",
    "\n",
    "- Let's organize folders of the dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c3814",
   "metadata": {
    "id": "qgqZQkV5IbXw"
   },
   "outputs": [],
   "source": [
    "!pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f4403",
   "metadata": {
    "id": "hpb5RbAAJQ8p"
   },
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "path_to_input = '/content/drive/MyDrive/Workshop/GreenHouseBot_data/input'\n",
    "splitfolders.ratio(path_to_input, output=\"/content/drive/MyDrive/Workshop/GreenHouseBot_data/\", seed=1337, ratio=(.8, 0,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6c373",
   "metadata": {
    "id": "sRx9LkA-MxT3"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def main_folder_move(src, dst):  \n",
    "    # Define the source and destination path\n",
    "    source = src\n",
    "    destination = dst\n",
    "    \n",
    "    # code to move the files from sub-folder to main folder.\n",
    "    files = os.listdir(source)\n",
    "    for file in files:\n",
    "        file_name = os.path.join(source, file)\n",
    "        shutil.move(file_name, destination)\n",
    "    print(\"Files Moved\")\n",
    "\n",
    "for folder_name in ['train/radish','train/cabbage','test/radish','test/cabbage']:\n",
    "    try:\n",
    "        root = \"/content/drive/MyDrive/Workshop/GreenHouseBot_data/\"\n",
    "        src = root + folder_name\n",
    "        dst = root + '/' + folder_name.split('/')[0]\n",
    "        main_folder_move(src, dst)\n",
    "\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aed12a",
   "metadata": {
    "id": "d03b11b4"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "- Let's create a custom <code>Dataset</code> class by subclassing the <code>Pytorch Dataset</code> class:\n",
    "    - We need just two essential fuctions <code>__len__</code> & <code>__getitem__</code> in our custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15501ad8",
   "metadata": {
    "id": "c27ee2ae"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # fix random seed\n",
    "\n",
    "class pytorch_data(Dataset):\n",
    "    \n",
    "    def __init__(self,data_dir,transform,data_type=\"train\"):      \n",
    "    \n",
    "        # Get Image File Names\n",
    "        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n",
    "\n",
    "        file_names =  [f for f in os.listdir(cdm_data) if os.path.isfile(os.path.join(cdm_data, f))]\n",
    "        #os.listdir(cdm_data) # get list of images in that directory  \n",
    "        idx_choose = np.random.choice(np.arange(len(file_names)), \n",
    "                                      100,\n",
    "                                      replace=False).tolist()\n",
    "        file_names_sample = [file_names[x] for x in idx_choose]\n",
    "        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n",
    "        \n",
    "        # Get Labels\n",
    "        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n",
    "        labels_df=pd.read_csv(labels_data, index_col=0)\n",
    "        labels_df['id'] = labels_df['id'].astype('str') \n",
    "        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n",
    "\n",
    "        self.labels = [labels_df.loc[filename].values[0] for filename in file_names_sample]  # obtained labels from df\n",
    "        self.transform = transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames) # size of dataset\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        # open image, apply transforms and return with label\n",
    "        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n",
    "        image = self.transform(image) # Apply Specific Transformation to Image\n",
    "        return image, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb66a4",
   "metadata": {
    "id": "f5f09aba"
   },
   "outputs": [],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "import torchvision.transforms as transforms\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((46,46))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b5a38",
   "metadata": {
    "id": "10d49db2"
   },
   "outputs": [],
   "source": [
    "# Define an object of the custom dataset for the train folder.\n",
    "data_dir = '/content/drive/MyDrive/Workshop/GreenHouseBot_data'\n",
    "img_dataset = pytorch_data(data_dir, data_transformer, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbea79",
   "metadata": {
    "id": "5bd149df"
   },
   "outputs": [],
   "source": [
    "# load an example tensor\n",
    "img,label=img_dataset[10]\n",
    "print(img.shape,torch.min(img),torch.max(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ce6df",
   "metadata": {
    "id": "8e97d072"
   },
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "- Among the training set, we need to evaluate the model on validation datasets to track the model's performance during training.\n",
    "- Let's use 20% of img_dataset for validation & use the rest as the training set, so we have a 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932173f8",
   "metadata": {
    "id": "5c611247"
   },
   "outputs": [],
   "source": [
    "len_img=len(img_dataset)\n",
    "len_train=int(0.8*len_img)\n",
    "len_val=len_img-len_train\n",
    "\n",
    "# Split Pytorch tensor\n",
    "train_ts,val_ts=random_split(img_dataset,\n",
    "                             [len_train,len_val]) # random split 80/20\n",
    "\n",
    "print(\"train dataset size:\", len(train_ts))\n",
    "print(\"validation dataset size:\", len(val_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231d427",
   "metadata": {
    "id": "abbf769b"
   },
   "outputs": [],
   "source": [
    "# getting the torch tensor image & target variable\n",
    "ii=-1\n",
    "for x,y in train_ts:\n",
    "    print(x.shape,y)\n",
    "    ii+=1\n",
    "    if(ii>5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5646174",
   "metadata": {
    "id": "dc33d52e"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_img(x,y,title=None):\n",
    "\n",
    "    npimg = x.numpy() # convert tensor to numpy array\n",
    "    npimg_tr=np.transpose(npimg, (1,2,0)) # Convert to H*W*C shape\n",
    "    fig = px.imshow(npimg_tr)\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.update_layout(title=title,height=300,margin={'l':10,'r':20,'b':10})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177144c5",
   "metadata": {
    "id": "803216d7"
   },
   "source": [
    "## Training subset examples\n",
    "\n",
    "- Some examples from our training data subset, with corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744316c9",
   "metadata": {
    "id": "b8ac0d91"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create grid of sample images \n",
    "grid_size=30\n",
    "rnd_inds=np.random.randint(0,len(train_ts),grid_size)\n",
    "print(\"image indices:\",rnd_inds)\n",
    "\n",
    "x_grid_train=[train_ts[i][0] for i in rnd_inds]\n",
    "y_grid_train=[train_ts[i][1] for i in rnd_inds]\n",
    "\n",
    "x_grid_train=utils.make_grid(x_grid_train, nrow=10, padding=2)\n",
    "print(x_grid_train.shape)\n",
    "    \n",
    "plot_img(x_grid_train,y_grid_train,'Training Subset Examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363cdc5",
   "metadata": {
    "id": "fffe55da"
   },
   "source": [
    "# Transforming the Dataset\n",
    "\n",
    "#### **<span style='color:#F1A424'>IMAGE AUGMENTATIONS</span>**\n",
    "\n",
    "- Among with pretrained models, image __transformation__ and __image augmentation__ are generally considered to be an essential parts of constructing deep learning models.\n",
    "- Using image transformations, we can expand our dataset or resize and normalise it to achieve better model performance.\n",
    "- Typical transformations include __horizontal__,__vertical flipping__, __rotation__, __resizing__.\n",
    "- We can use various image transformations for our binary classification model without making label changes; we can flip/rotate a image but it will remain the same class.\n",
    "- We can use the torchvision module to perform image transformations during the training process.\n",
    "\n",
    "#### **<span style='color:#F1A424'>TRAINING DATA AUGMENTATIONS</span>**\n",
    "- transforms.RandomHorizontalFlip(p=0.5): Flips the image horizontally with the probability of 0.5\n",
    "- transforms.RandomVerticalFlip(p=0.5) : Flips the image vertically  \" \n",
    "- transforms.RandomRotation(45) : Rotates the images in the range of (-45,45) degrees.\n",
    "- transforms.RandomResizedCrop(96,scale=(0.8,1.0),ratio=(1.0,1.0)) : Randomly square crops the image in the range of [72,96], followed by a resize to 96x96, which is the original pixel size of our image data.\n",
    "- transforms.ToTensor() : Converts to Tensor & Normalises as shown above already.\n",
    "\n",
    "Read more about augmentation [here](https://link.medium.com/BRSwusrU5sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c48672",
   "metadata": {
    "id": "9d396b35"
   },
   "outputs": [],
   "source": [
    "# Define the following transformations for the training dataset\n",
    "tr_transf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomVerticalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(45), \n",
    "    # ADD YOUR AUGMENTATION HERE        \n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862c352",
   "metadata": {
    "id": "9c904c3e"
   },
   "outputs": [],
   "source": [
    "# For the validation dataset, we don't need any augmentation; simply convert images into tensors\n",
    "val_transf = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# After defining the transformations, overwrite the transform functions of train_ts, val_ts\n",
    "train_ts.transform=tr_transf\n",
    "val_ts.transform=val_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41cb7e",
   "metadata": {
    "id": "0cda6cfe"
   },
   "source": [
    "# Creating Dataloaders\n",
    "\n",
    "- Ready to create a PyTorch Dataloader. If we don't use __Dataloaders__, we have to write code to loop over datasets & extract a data batch; automated.\n",
    "- We need to define a __batch_size__ : The number of images extracted from the dataset each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e079b",
   "metadata": {
    "id": "70376d95"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training DataLoader\n",
    "train_dl = DataLoader(train_ts,\n",
    "                      batch_size=8, \n",
    "                      shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dl = DataLoader(val_ts,\n",
    "                    batch_size=8,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e5c67",
   "metadata": {
    "id": "aa54d291"
   },
   "outputs": [],
   "source": [
    "for x,y in train_dl:\n",
    "    print(x.shape,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad8026",
   "metadata": {
    "id": "dc095d7e"
   },
   "source": [
    "# Binary Classifier CNN Model\n",
    "\n",
    "- Model is comprised of\n",
    "  - **<span style='color:#F1A424'>four CNN</span>** **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">Conv2D</mark>** layers with a **<span style='color:#F1A424'>pooling layer</span>** **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">max_pool2D</mark>** added between each layer \n",
    "  - Two **<span style='color:#F1A424'>fully connected</span>** layers **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">fc</mark>**, with a **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">dropout</mark>** layer between the two layers\n",
    "  - **<span style='color:#F1A424'>log_softmax</span>** is used as the activation function for the final layer of the **<span style='color:#F1A424'>binary classifier</span>**\n",
    "- PyTorch allows us to create a custom class with <code>nn.Module</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25ac26",
   "metadata": {
    "id": "d76db300"
   },
   "outputs": [],
   "source": [
    "# Useful Function to calculate the output size of a CNN layer\n",
    "# before making it an input into the linear layer\n",
    "\n",
    "def findConv2dOutShape(hin,win,conv,pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "\n",
    "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool\n",
    "        wout/=pool\n",
    "    return int(hout),int(wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ded21c",
   "metadata": {
    "id": "87c82d53"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Neural Network\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "    \n",
    "        Cin,Hin,Win=params[\"shape_in\"]\n",
    "        init_f=params[\"initial_filters\"] \n",
    "        num_fc1=params[\"num_fc1\"]  \n",
    "        num_classes=params[\"num_classes\"] \n",
    "        self.dropout_rate=params[\"dropout_rate\"] \n",
    "        \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
    "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv4)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten=h*w*8*init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def forward(self,X):\n",
    "        \n",
    "        # Convolution & Pool Layers\n",
    "        X = F.relu(self.conv1(X)); X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X)); X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X));X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv4(X));X = F.max_pool2d(X, 2, 2)\n",
    "\n",
    "        X = X.view(-1, self.num_flatten)\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, self.dropout_rate)\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89cc6c",
   "metadata": {
    "id": "435bce0b"
   },
   "outputs": [],
   "source": [
    "# Neural Network Predefined Parameters\n",
    "params_model={\n",
    "        \"shape_in\": (3,46,46), \n",
    "        \"initial_filters\": 8,    \n",
    "        \"num_fc1\": 100,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"num_classes\": 2}\n",
    "\n",
    "# Create instantiation of Network class\n",
    "cnn_model = Network(params_model)\n",
    "\n",
    "# define computation hardware approach (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2df13",
   "metadata": {
    "id": "44639481"
   },
   "source": [
    "# Defining a Loss Function\n",
    "\n",
    "- Loss Functions are one of the key pieces of an effective deep learning solution.\n",
    "- Pytorch uses <code>loss functions</code> to determine how it will update the network to reach the desired solution.\n",
    "- The standard loss function for classification tasks is __cross entropy loss__ or __logloss__\n",
    "- When defining a loss function, we need to consider, the number of model outputs and their activation functions.\n",
    "- For binary classification tasks, we can choose one or two outputs.\n",
    "- It is recommended to use __log_softmax__ as it is easier to expand to multiclass classification; PyTorch combines the log and softmax operations into one function, due to numerical stability and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca010c4e",
   "metadata": {
    "id": "984144d8"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a6172",
   "metadata": {
    "id": "d4b3b041"
   },
   "source": [
    "#  Defining an Optimiser\n",
    "\n",
    "- Training the network involves passing data through the network:\n",
    "    - Using the **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">loss function</mark>** to **<span style='color:#F1A424'>determine the difference between the prediction & true value</span>**\n",
    "    - Which is then followed by using of that info to **<span style='color:#F1A424'>update the weights</span>** of the network \n",
    "    - In an attempt to **<span style='color:#F1A424'>make the loss function return as small of a loss as possible, performing updates on the neural network</span>**, an **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">optimiser</mark>** is used\n",
    "- The <code>torch.optim</code> contains implementations of common optimisers\n",
    "- The **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">optimiser</mark>** will **<span style='color:#F1A424'>hold the current state and will update the parameters based on the computed gradients</mark>**\n",
    "- For binary classification taskss, __SGD__, __Adam__ Optimisers are commonly used, let's use the latter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc061d",
   "metadata": {
    "id": "c1a6bc8a"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "opt = optim.Adam(cnn_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546eb53",
   "metadata": {
    "id": "f1105ff9"
   },
   "source": [
    "#  Training the Model\n",
    "\n",
    "## Helper functions\n",
    "\n",
    "- The main training loop function <code>train_val</code> will utiliser three functions:\n",
    "    - <code>get_lr</code> : get the learning rate as it is adjusted \n",
    "    - <code>loss_batch</code> : get the loss value for the particular batch\n",
    "    - <code>loss_epoch</code> : get the entire loss for an epoch iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c11dca",
   "metadata": {
    "id": "47a0f516"
   },
   "outputs": [],
   "source": [
    "''' Helper Functions'''\n",
    "\n",
    "# Function to get the learning rate\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Function to compute the loss value per batch of data\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    \n",
    "    loss = loss_func(output, target) # get loss\n",
    "    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n",
    "    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "# Compute the loss value & performance metric for the entire dataset (epoch)\n",
    "def loss_epoch(model,loss_func,dataset_dl,check=False,opt=None):\n",
    "    \n",
    "    run_loss=0.0 \n",
    "    t_metric=0.0\n",
    "    len_data=len(dataset_dl.dataset)\n",
    "\n",
    "    # internal loop over dataset\n",
    "    for xb, yb in dataset_dl:\n",
    "        # move batch to device\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        output=model(xb) # get model output\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n",
    "        run_loss+=loss_b        # update running loss\n",
    "\n",
    "        if metric_b is not None: # update running metric\n",
    "            t_metric+=metric_b\n",
    "\n",
    "        # break the loop in case of sanity check\n",
    "        if check is True:\n",
    "            break\n",
    "    \n",
    "    loss=run_loss/float(len_data)  # average loss value\n",
    "    metric=t_metric/float(len_data) # average metric value\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9c54",
   "metadata": {
    "id": "aef864ec"
   },
   "source": [
    "## Main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a405a3",
   "metadata": {
    "id": "1653a855"
   },
   "outputs": [],
   "source": [
    "def train_val(model, params,verbose=False):\n",
    "    \n",
    "    # Get the parameters\n",
    "    epochs=params[\"epochs\"]\n",
    "    loss_func=params[\"f_loss\"]\n",
    "    opt=params[\"optimiser\"]\n",
    "    train_dl=params[\"train\"]\n",
    "    val_dl=params[\"val\"]\n",
    "    check=params[\"check\"]\n",
    "    lr_scheduler=params[\"lr_change\"]\n",
    "    weight_path=params[\"weight_path\"]\n",
    "    \n",
    "    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n",
    "    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n",
    "    best_loss=float('inf') # initialize best loss to a large value\n",
    "    \n",
    "    # main loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        ''' Get the Learning Rate '''\n",
    "        current_lr=get_lr(opt)\n",
    "        if(verbose):\n",
    "            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n",
    "        \n",
    "        ''' Train the Model on the Training Set '''\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,check,opt)\n",
    "\n",
    "        ''' Collect loss and metric for training dataset ''' \n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        \n",
    "        ''' Evaluate model on validation dataset '''\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl,check)\n",
    "        \n",
    "        # store best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # store weights into a local file\n",
    "            torch.save(model.state_dict(), weight_path)\n",
    "            if(verbose):\n",
    "                print(\"Copied best model weights!\")\n",
    "        \n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        \n",
    "        # learning rate schedule\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            if(verbose):\n",
    "                print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts) \n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n",
    "            print(\"-\"*10) \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca4dec",
   "metadata": {
    "id": "1d2991f3"
   },
   "outputs": [],
   "source": [
    "params_train={\n",
    " \"train\": train_dl,\"val\": val_dl,\n",
    " \"epochs\": 50,\n",
    " \"optimiser\": optim.Adam(cnn_model.parameters(),\n",
    "                         lr=3e-4),\n",
    " \"lr_change\": ReduceLROnPlateau(opt,\n",
    "                                mode='min',\n",
    "                                factor=0.5,\n",
    "                                patience=20,\n",
    "                                verbose=0),\n",
    " \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n",
    " \"weight_path\": \"weights.pt\",\n",
    " \"check\": False, \n",
    "}\n",
    "\n",
    "''' Actual Train / Evaluation of CNN Model '''\n",
    "# train and validate the model\n",
    "cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50630ef",
   "metadata": {
    "id": "379473de"
   },
   "outputs": [],
   "source": [
    "# Train-Validation Progress\n",
    "epochs=params_train[\"epochs\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,subplot_titles=['lost_hist','metric_hist'])\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=loss_hist[\"train\"],name='loss_hist[\"train\"]'),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=loss_hist[\"val\"],name='loss_hist[\"val\"]'),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=metric_hist[\"train\"],name='metric_hist[\"train\"]'),row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=metric_hist[\"val\"],name='metric_hist[\"val\"]'),row=1, col=2)\n",
    "fig.update_layout(template='plotly_white')\n",
    "fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0},height=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77295b09",
   "metadata": {
    "id": "yMRqmRq8-TMz"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(cnn_model, '/content/drive/MyDrive/Workshop/cnn_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0797cc7",
   "metadata": {
    "id": "ysVffR0jh_3K"
   },
   "source": [
    "## Function for bot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafc7d5",
   "metadata": {
    "id": "JWWUCSB0iDCN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "    \n",
    "        Cin,Hin,Win=params[\"shape_in\"]\n",
    "        init_f=params[\"initial_filters\"] \n",
    "        num_fc1=params[\"num_fc1\"]  \n",
    "        num_classes=params[\"num_classes\"] \n",
    "        self.dropout_rate=params[\"dropout_rate\"] \n",
    "        \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
    "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv4)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten=h*w*8*init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def forward(self,X):\n",
    "        \n",
    "        # Convolution & Pool Layers\n",
    "        X = F.relu(self.conv1(X)); X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X)); X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X));X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv4(X));X = F.max_pool2d(X, 2, 2)\n",
    "\n",
    "        X = X.view(-1, self.num_flatten)\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, self.dropout_rate)\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n",
    "def run_model (image_path, model_path):\n",
    "\n",
    "    cnn_model = torch.load(model_path)\n",
    "\n",
    "    device = torch.device(str(\"cuda:0\")if torch.cuda.is_available() else \"cpu\") \n",
    "    cnn_model = cnn_model.to(device) \n",
    "    cnn_model.eval()\n",
    "\n",
    "    image_path = image_path\n",
    "    image = plt.imread(image_path)\n",
    "    image = np.array(image)\n",
    "    print(image.shape)\n",
    "\n",
    "    my_transforms = transforms.Compose([transforms.ToTensor(),transforms.Resize((46,46))])\n",
    "    image_tensor = my_transforms(image).unsqueeze(0) \n",
    "    image_tensor = image_tensor.to(device)\n",
    "    output=cnn_model(image_tensor)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "\n",
    "    return (pred[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317f93",
   "metadata": {
    "id": "yMIkfl25jUEs"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/Workshop/cnn_model.pt'\n",
    "image_path = '/content/drive/MyDrive/Workshop/GreenHouseBot_data/test/IMG_0268.jpeg'\n",
    "\n",
    "pred = run_model(image_path, model_path)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 455.740821,
   "end_time": "2022-07-08T12:23:26.435019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-08T12:15:50.694198",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
