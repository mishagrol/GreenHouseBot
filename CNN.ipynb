{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda8029",
   "metadata": {},
   "source": [
    "# **<span style='color:#F1A424'>Innavation Workshop - 2022. Quick success</span>**\n",
    "#### **<span style='color:#F1A424'>Practical part of Quick Success, Skoltech, Moscow-2022</span>**\n",
    "\n",
    "**Instructors:** Elizveta Kiseleva, Mikhail Gasanov, Anna Petrovskaia\n",
    "\n",
    "To prepare this seminar the following resources were used:\n",
    "\n",
    "[kaggle](https://www.kaggle.com/code/shtrausslearning/pytorch-cnn-binary-image-classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aee3eb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.919368,
     "end_time": "2022-07-08T12:16:00.854440",
     "exception": false,
     "start_time": "2022-07-08T12:15:58.935072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torchvision import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953087d",
   "metadata": {
    "papermill": {
     "duration": 12.035314,
     "end_time": "2022-07-08T12:16:12.900592",
     "exception": false,
     "start_time": "2022-07-08T12:16:00.865278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae24b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0d1bf",
   "metadata": {
    "papermill": {
     "duration": 0.010664,
     "end_time": "2022-07-08T12:16:22.057864",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.047200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84b1cd",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "- Load the dataset information file; train_labels.csv, it contains a reference to an image ID (id) & its classification allocation (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0c183",
   "metadata": {
    "papermill": {
     "duration": 0.430569,
     "end_time": "2022-07-08T12:16:22.499678",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.069109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('c)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03601e93",
   "metadata": {
    "papermill": {
     "duration": 0.020973,
     "end_time": "2022-07-08T12:16:22.532378",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.511405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d85947",
   "metadata": {
    "papermill": {
     "duration": 0.010775,
     "end_time": "2022-07-08T12:16:22.554174",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.543399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check for duplicate entries\n",
    "\n",
    "- Check if the dataset contains any duplicates, if there is we should drop them, which we have none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15745d08",
   "metadata": {
    "papermill": {
     "duration": 0.095612,
     "end_time": "2022-07-08T12:16:22.660682",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.565070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No duplicate ids found\n",
    "labels_df[labels_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0649b3",
   "metadata": {
    "papermill": {
     "duration": 0.011252,
     "end_time": "2022-07-08T12:16:22.683736",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.672484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Target feature class balance\n",
    "- Let's check the number of object in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c4048",
   "metadata": {
    "papermill": {
     "duration": 0.024014,
     "end_time": "2022-07-08T12:16:22.719230",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.695216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005dc0be",
   "metadata": {
    "papermill": {
     "duration": 0.010984,
     "end_time": "2022-07-08T12:16:22.741405",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.730421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset preview\n",
    "\n",
    "- Let's also visualise the dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64676b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath =\"/mnt/bulky/apetrovskaya/workshop/train/\" # training data is stored in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_names_raw = os.listdir(imgpath)\n",
    "train_img_names  = [f.split('.')[0] for f in train_img_names_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df[labels_df.id.isin(train_img_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24197005",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.031843,
     "end_time": "2022-07-08T12:16:22.784427",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.752584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "malignant = labels_df.loc[labels_df['label']==1]['id'].values    # get the ids of malignant cases\n",
    "normal = labels_df.loc[labels_df['label']==0]['id'].values       # get the ids of the normal cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe616f8",
   "metadata": {
    "papermill": {
     "duration": 3.689737,
     "end_time": "2022-07-08T12:16:26.485430",
     "exception": false,
     "start_time": "2022-07-08T12:16:22.795693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrows,ncols=6,15\n",
    "fig,ax = plt.subplots(nrows,ncols,figsize=(15,6))\n",
    "plt.subplots_adjust(wspace=0, hspace=0) \n",
    "for i,j in enumerate(malignant[:nrows*ncols]):\n",
    "    fname = os.path.join(imgpath ,j +'.tif')\n",
    "    img = Image.open(fname)\n",
    "    idcol = ImageDraw.Draw(img)\n",
    "    idcol.rectangle(((0,0),(95,95)),outline='red')\n",
    "    plt.subplot(nrows, ncols, i+1) \n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100968d",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 3.632133,
     "end_time": "2022-07-08T12:16:30.139769",
     "exception": false,
     "start_time": "2022-07-08T12:16:26.507636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "nrows,ncols=6,15\n",
    "for i,j in enumerate(normal[:nrows*ncols]):\n",
    "    fname = os.path.join(imgpath ,j +'.tif')\n",
    "    img = Image.open(fname)\n",
    "    idcol = ImageDraw.Draw(img)\n",
    "    idcol.rectangle(((0,0),(95,95)),outline='green')\n",
    "    plt.subplot(nrows, ncols, i+1) \n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d26f2",
   "metadata": {
    "papermill": {
     "duration": 0.042454,
     "end_time": "2022-07-08T12:16:30.216942",
     "exception": false,
     "start_time": "2022-07-08T12:16:30.174488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "- Let's create a custom <code>Dataset</code> class by subclassing the <code>Pytorch Dataset</code> class:\n",
    "    - We need just two essential fuctions <code>__len__</code> & <code>__getitem__</code> in our custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc688592",
   "metadata": {
    "papermill": {
     "duration": 0.050431,
     "end_time": "2022-07-08T12:16:30.301260",
     "exception": false,
     "start_time": "2022-07-08T12:16:30.250829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # fix random seed\n",
    "\n",
    "class pytorch_data(Dataset):\n",
    "    \n",
    "    def __init__(self,data_dir,transform,data_type=\"train\"):      \n",
    "    \n",
    "        # Get Image File Names\n",
    "        cdm_data=os.path.join(data_dir,data_type)  # directory of files\n",
    "        \n",
    "        file_names = os.listdir(cdm_data) # get list of images in that directory  \n",
    "        idx_choose = np.random.choice(np.arange(len(file_names)), \n",
    "                                      4000,\n",
    "                                      replace=False).tolist()\n",
    "        file_names_sample = [file_names[x] for x in idx_choose]\n",
    "        self.full_filenames = [os.path.join(cdm_data, f) for f in file_names_sample]   # get the full path to images\n",
    "        \n",
    "        # Get Labels\n",
    "        labels_data=os.path.join(data_dir,\"train_labels.csv\") \n",
    "        labels_df=pd.read_csv(labels_data)\n",
    "        labels_df.set_index(\"id\", inplace=True) # set data frame index to id\n",
    "        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in file_names_sample]  # obtained labels from df\n",
    "        self.transform = transform\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.full_filenames) # size of dataset\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        # open image, apply transforms and return with label\n",
    "        image = Image.open(self.full_filenames[idx])  # Open Image with PIL\n",
    "        image = self.transform(image) # Apply Specific Transformation to Image\n",
    "        return image, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80dd3f",
   "metadata": {
    "papermill": {
     "duration": 0.04279,
     "end_time": "2022-07-08T12:16:30.377488",
     "exception": false,
     "start_time": "2022-07-08T12:16:30.334698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define transformation that converts a PIL image into PyTorch tensors\n",
    "import torchvision.transforms as transforms\n",
    "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((46,46))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39612f92",
   "metadata": {
    "papermill": {
     "duration": 6.34498,
     "end_time": "2022-07-08T12:16:36.755923",
     "exception": false,
     "start_time": "2022-07-08T12:16:30.410943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an object of the custom dataset for the train folder.\n",
    "data_dir = '/mnt/bulky/apetrovskaya/workshop/'\n",
    "img_dataset = pytorch_data(data_dir, data_transformer, \"train\") # Histopathalogic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2bdff",
   "metadata": {
    "papermill": {
     "duration": 0.078726,
     "end_time": "2022-07-08T12:16:36.868833",
     "exception": false,
     "start_time": "2022-07-08T12:16:36.790107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load an example tensor\n",
    "img,label=img_dataset[10]\n",
    "print(img.shape,torch.min(img),torch.max(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910946d6",
   "metadata": {
    "papermill": {
     "duration": 0.033473,
     "end_time": "2022-07-08T12:16:36.937685",
     "exception": false,
     "start_time": "2022-07-08T12:16:36.904212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "- Among the training set, we need to evaluate the model on validation datasets to track the model's performance during training.\n",
    "- Let's use 20% of img_dataset for validation & use the rest as the training set, so we have a 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b394a",
   "metadata": {
    "papermill": {
     "duration": 0.047301,
     "end_time": "2022-07-08T12:16:37.018401",
     "exception": false,
     "start_time": "2022-07-08T12:16:36.971100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_img=len(img_dataset)\n",
    "len_train=int(0.8*len_img)\n",
    "len_val=len_img-len_train\n",
    "\n",
    "# Split Pytorch tensor\n",
    "train_ts,val_ts=random_split(img_dataset,\n",
    "                             [len_train,len_val]) # random split 80/20\n",
    "\n",
    "print(\"train dataset size:\", len(train_ts))\n",
    "print(\"validation dataset size:\", len(val_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1deac",
   "metadata": {
    "papermill": {
     "duration": 0.131789,
     "end_time": "2022-07-08T12:16:37.185124",
     "exception": false,
     "start_time": "2022-07-08T12:16:37.053335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the torch tensor image & target variable\n",
    "ii=-1\n",
    "for x,y in train_ts:\n",
    "    print(x.shape,y)\n",
    "    ii+=1\n",
    "    if(ii>5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c62d4",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 2.749854,
     "end_time": "2022-07-08T12:16:39.970699",
     "exception": false,
     "start_time": "2022-07-08T12:16:37.220845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_img(x,y,title=None):\n",
    "\n",
    "    npimg = x.numpy() # convert tensor to numpy array\n",
    "    npimg_tr=np.transpose(npimg, (1,2,0)) # Convert to H*W*C shape\n",
    "    fig = px.imshow(npimg_tr)\n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.update_layout(title=title,height=300,margin={'l':10,'r':20,'b':10})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b37623",
   "metadata": {
    "papermill": {
     "duration": 0.034548,
     "end_time": "2022-07-08T12:16:40.041230",
     "exception": false,
     "start_time": "2022-07-08T12:16:40.006682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training subset examples\n",
    "\n",
    "- Some examples from our training data subset, with corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fee78",
   "metadata": {
    "papermill": {
     "duration": 1.523795,
     "end_time": "2022-07-08T12:16:41.599407",
     "exception": false,
     "start_time": "2022-07-08T12:16:40.075612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create grid of sample images \n",
    "grid_size=30\n",
    "rnd_inds=np.random.randint(0,len(train_ts),grid_size)\n",
    "print(\"image indices:\",rnd_inds)\n",
    "\n",
    "x_grid_train=[train_ts[i][0] for i in rnd_inds]\n",
    "y_grid_train=[train_ts[i][1] for i in rnd_inds]\n",
    "\n",
    "x_grid_train=utils.make_grid(x_grid_train, nrow=10, padding=2)\n",
    "print(x_grid_train.shape)\n",
    "    \n",
    "plot_img(x_grid_train,y_grid_train,'Training Subset Examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5217dc",
   "metadata": {
    "papermill": {
     "duration": 0.036902,
     "end_time": "2022-07-08T12:16:41.674385",
     "exception": false,
     "start_time": "2022-07-08T12:16:41.637483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation subset examples\n",
    "\n",
    "- Some examples from the validation subset, with corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a1873",
   "metadata": {
    "papermill": {
     "duration": 0.505127,
     "end_time": "2022-07-08T12:16:42.217260",
     "exception": false,
     "start_time": "2022-07-08T12:16:41.712133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_size=30\n",
    "rnd_inds=np.random.randint(0,len(val_ts),grid_size)\n",
    "print(\"image indices:\",rnd_inds)\n",
    "x_grid_val=[val_ts[i][0] for i in range(grid_size)]\n",
    "y_grid_val=[val_ts[i][1] for i in range(grid_size)]\n",
    "\n",
    "x_grid_val=utils.make_grid(x_grid_val, nrow=10, padding=2)\n",
    "print(x_grid_val.shape)\n",
    "\n",
    "plot_img(x_grid_val,y_grid_val,'Validation Dataset Preview')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adad762",
   "metadata": {
    "papermill": {
     "duration": 0.073981,
     "end_time": "2022-07-08T12:16:42.334396",
     "exception": false,
     "start_time": "2022-07-08T12:16:42.260415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transforming the Dataset\n",
    "\n",
    "#### **<span style='color:#F1A424'>IMAGE AUGMENTATIONS</span>**\n",
    "\n",
    "- Among with pretrained models, image __transformation__ and __image augmentation__ are generally considered to be an essential parts of constructing deep learning models.\n",
    "- Using image transformations, we can expand our dataset or resize and normalise it to achieve better model performance.\n",
    "- Typical transformations include __horizontal__,__vertical flipping__, __rotation__, __resizing__.\n",
    "- We can use various image transformations for our binary classification model without making label changes; we can flip/rotate a image but it will remain the same class.\n",
    "- We can use the torchvision module to perform image transformations during the training process.\n",
    "\n",
    "#### **<span style='color:#F1A424'>TRAINING DATA AUGMENTATIONS</span>**\n",
    "- transforms.RandomHorizontalFlip(p=0.5): Flips the image horizontally with the probability of 0.5\n",
    "- transforms.RandomVerticalFlip(p=0.5) : Flips the image vertically  \" \n",
    "- transforms.RandomRotation(45) : Rotates the images in the range of (-45,45) degrees.\n",
    "- transforms.RandomResizedCrop(96,scale=(0.8,1.0),ratio=(1.0,1.0)) : Randomly square crops the image in the range of [72,96], followed by a resize to 96x96, which is the original pixel size of our image data.\n",
    "- transforms.ToTensor() : Converts to Tensor & Normalises as shown above already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a4e7a",
   "metadata": {
    "papermill": {
     "duration": 0.049892,
     "end_time": "2022-07-08T12:16:42.427342",
     "exception": false,
     "start_time": "2022-07-08T12:16:42.377450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the following transformations for the training dataset\n",
    "tr_transf = transforms.Compose([\n",
    "#     transforms.Resize((40,40)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomVerticalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(45),         \n",
    "#     transforms.RandomResizedCrop(50,scale=(0.8,1.0),ratio=(1.0,1.0)),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a653d2e",
   "metadata": {
    "papermill": {
     "duration": 0.049701,
     "end_time": "2022-07-08T12:16:42.518399",
     "exception": false,
     "start_time": "2022-07-08T12:16:42.468698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the validation dataset, we don't need any augmentation; simply convert images into tensors\n",
    "val_transf = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# After defining the transformations, overwrite the transform functions of train_ts, val_ts\n",
    "train_ts.transform=tr_transf\n",
    "val_ts.transform=val_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6274d",
   "metadata": {
    "papermill": {
     "duration": 0.040694,
     "end_time": "2022-07-08T12:16:42.599265",
     "exception": false,
     "start_time": "2022-07-08T12:16:42.558571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating Dataloaders\n",
    "\n",
    "- Ready to create a PyTorch Dataloader. If we don't use __Dataloaders__, we have to write code to loop over datasets & extract a data batch; automated.\n",
    "- We need to define a __batch_size__ : The number of images extracted from the dataset each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c661ec9",
   "metadata": {
    "papermill": {
     "duration": 0.050787,
     "end_time": "2022-07-08T12:16:42.690403",
     "exception": false,
     "start_time": "2022-07-08T12:16:42.639616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training DataLoader\n",
    "train_dl = DataLoader(train_ts,\n",
    "                      batch_size=32, \n",
    "                      shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dl = DataLoader(val_ts,\n",
    "                    batch_size=32,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c40f5",
   "metadata": {
    "papermill": {
     "duration": 0.39883,
     "end_time": "2022-07-08T12:16:43.129641",
     "exception": false,
     "start_time": "2022-07-08T12:16:42.730811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x,y in train_dl:\n",
    "    print(x.shape,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5695c4",
   "metadata": {
    "papermill": {
     "duration": 0.041771,
     "end_time": "2022-07-08T12:16:43.213065",
     "exception": false,
     "start_time": "2022-07-08T12:16:43.171294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Binary Classifier CNN Model\n",
    "\n",
    "- Model is comprised of\n",
    "  - **<span style='color:#F1A424'>four CNN</span>** **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">Conv2D</mark>** layers with a **<span style='color:#F1A424'>pooling layer</span>** **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">max_pool2D</mark>** added between each layer \n",
    "  - Two **<span style='color:#F1A424'>fully connected</span>** layers **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">fc</mark>**, with a **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">dropout</mark>** layer between the two layers\n",
    "  - **<span style='color:#F1A424'>log_softmax</span>** is used as the activation function for the final layer of the **<span style='color:#F1A424'>binary classifier</span>**\n",
    "- PyTorch allows us to create a custom class with <code>nn.Module</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ee3fa",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.052431,
     "end_time": "2022-07-08T12:16:43.306681",
     "exception": false,
     "start_time": "2022-07-08T12:16:43.254250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful Function to calculate the output size of a CNN layer\n",
    "# before making it an input into the linear layer\n",
    "\n",
    "def findConv2dOutShape(hin,win,conv,pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size=conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "\n",
    "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool\n",
    "        wout/=pool\n",
    "    return int(hout),int(wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b923d",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.058786,
     "end_time": "2022-07-08T12:16:43.406385",
     "exception": false,
     "start_time": "2022-07-08T12:16:43.347599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Neural Network\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    # Network Initialisation\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "    \n",
    "        Cin,Hin,Win=params[\"shape_in\"]\n",
    "        init_f=params[\"initial_filters\"] \n",
    "        num_fc1=params[\"num_fc1\"]  \n",
    "        num_classes=params[\"num_classes\"] \n",
    "        self.dropout_rate=params[\"dropout_rate\"] \n",
    "        \n",
    "        # Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
    "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h,w,self.conv4)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten=h*w*8*init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "\n",
    "    def forward(self,X):\n",
    "        \n",
    "        # Convolution & Pool Layers\n",
    "        X = F.relu(self.conv1(X)); X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X)); X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv3(X));X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv4(X));X = F.max_pool2d(X, 2, 2)\n",
    "\n",
    "        X = X.view(-1, self.num_flatten)\n",
    "        \n",
    "        X = F.relu(self.fc1(X))\n",
    "        X=F.dropout(X, self.dropout_rate)\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79eae5d",
   "metadata": {
    "papermill": {
     "duration": 2.967967,
     "end_time": "2022-07-08T12:16:46.414727",
     "exception": false,
     "start_time": "2022-07-08T12:16:43.446760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural Network Predefined Parameters\n",
    "params_model={\n",
    "        \"shape_in\": (3,46,46), \n",
    "        \"initial_filters\": 8,    \n",
    "        \"num_fc1\": 100,\n",
    "        \"dropout_rate\": 0.25,\n",
    "        \"num_classes\": 2}\n",
    "\n",
    "# Create instantiation of Network class\n",
    "cnn_model = Network(params_model)\n",
    "\n",
    "# define computation hardware approach (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ad142",
   "metadata": {
    "papermill": {
     "duration": 0.071744,
     "end_time": "2022-07-08T12:16:46.566399",
     "exception": false,
     "start_time": "2022-07-08T12:16:46.494655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Defining a Loss Function\n",
    "\n",
    "- Loss Functions are one of the key pieces of an effective deep learning solution.\n",
    "- Pytorch uses <code>loss functions</code> to determine how it will update the network to reach the desired solution.\n",
    "- The standard loss function for classification tasks is __cross entropy loss__ or __logloss__\n",
    "- When defining a loss function, we need to consider, the number of model outputs and their activation functions.\n",
    "- For binary classification tasks, we can choose one or two outputs.\n",
    "- It is recommended to use __log_softmax__ as it is easier to expand to multiclass classification; PyTorch combines the log and softmax operations into one function, due to numerical stability and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1e9f5",
   "metadata": {
    "papermill": {
     "duration": 0.091538,
     "end_time": "2022-07-08T12:16:46.731203",
     "exception": false,
     "start_time": "2022-07-08T12:16:46.639665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff024d4",
   "metadata": {
    "papermill": {
     "duration": 0.044821,
     "end_time": "2022-07-08T12:16:46.838742",
     "exception": false,
     "start_time": "2022-07-08T12:16:46.793921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Defining an Optimiser\n",
    "\n",
    "- Training the network involves passing data through the network:\n",
    "    - Using the **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">loss function</mark>** to **<span style='color:#F1A424'>determine the difference between the prediction & true value</span>**\n",
    "    - Which is then followed by using of that info to **<span style='color:#F1A424'>update the weights</span>** of the network \n",
    "    - In an attempt to **<span style='color:#F1A424'>make the loss function return as small of a loss as possible, performing updates on the neural network</span>**, an **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">optimiser</mark>** is used\n",
    "- The <code>torch.optim</code> contains implementations of common optimisers\n",
    "- The **<mark style=\"background-color:#F1C40F;color:white;border-radius:5px;opacity:0.9\">optimiser</mark>** will **<span style='color:#F1A424'>hold the current state and will update the parameters based on the computed gradients</mark>**\n",
    "- For binary classification taskss, __SGD__, __Adam__ Optimisers are commonly used, let's use the latter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d09a3a",
   "metadata": {
    "papermill": {
     "duration": 0.055168,
     "end_time": "2022-07-08T12:16:46.939112",
     "exception": false,
     "start_time": "2022-07-08T12:16:46.883944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "opt = optim.Adam(cnn_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff8986",
   "metadata": {
    "papermill": {
     "duration": 0.045656,
     "end_time": "2022-07-08T12:16:47.029601",
     "exception": false,
     "start_time": "2022-07-08T12:16:46.983945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Training the Model\n",
    "\n",
    "## Helper functions\n",
    "\n",
    "- The main training loop function <code>train_val</code> will utiliser three functions:\n",
    "    - <code>get_lr</code> : get the learning rate as it is adjusted \n",
    "    - <code>loss_batch</code> : get the loss value for the particular batch\n",
    "    - <code>loss_epoch</code> : get the entire loss for an epoch iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef44cd",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.065946,
     "end_time": "2022-07-08T12:16:47.142241",
     "exception": false,
     "start_time": "2022-07-08T12:16:47.076295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Helper Functions'''\n",
    "\n",
    "# Function to get the learning rate\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Function to compute the loss value per batch of data\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    \n",
    "    loss = loss_func(output, target) # get loss\n",
    "    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n",
    "    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "# Compute the loss value & performance metric for the entire dataset (epoch)\n",
    "def loss_epoch(model,loss_func,dataset_dl,check=False,opt=None):\n",
    "    \n",
    "    run_loss=0.0 \n",
    "    t_metric=0.0\n",
    "    len_data=len(dataset_dl.dataset)\n",
    "\n",
    "    # internal loop over dataset\n",
    "    for xb, yb in dataset_dl:\n",
    "        # move batch to device\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        output=model(xb) # get model output\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n",
    "        run_loss+=loss_b        # update running loss\n",
    "\n",
    "        if metric_b is not None: # update running metric\n",
    "            t_metric+=metric_b\n",
    "\n",
    "        # break the loop in case of sanity check\n",
    "        if check is True:\n",
    "            break\n",
    "    \n",
    "    loss=run_loss/float(len_data)  # average loss value\n",
    "    metric=t_metric/float(len_data) # average metric value\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaad7e",
   "metadata": {
    "papermill": {
     "duration": 0.044603,
     "end_time": "2022-07-08T12:16:47.231922",
     "exception": false,
     "start_time": "2022-07-08T12:16:47.187319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daf8ad",
   "metadata": {
    "papermill": {
     "duration": 0.279293,
     "end_time": "2022-07-08T12:16:47.557034",
     "exception": false,
     "start_time": "2022-07-08T12:16:47.277741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_val(model, params,verbose=False):\n",
    "    \n",
    "    # Get the parameters\n",
    "    epochs=params[\"epochs\"]\n",
    "    loss_func=params[\"f_loss\"]\n",
    "    opt=params[\"optimiser\"]\n",
    "    train_dl=params[\"train\"]\n",
    "    val_dl=params[\"val\"]\n",
    "    check=params[\"check\"]\n",
    "    lr_scheduler=params[\"lr_change\"]\n",
    "    weight_path=params[\"weight_path\"]\n",
    "    \n",
    "    loss_history={\"train\": [],\"val\": []} # history of loss values in each epoch\n",
    "    metric_history={\"train\": [],\"val\": []} # histroy of metric values in each epoch\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\n",
    "    best_loss=float('inf') # initialize best loss to a large value\n",
    "    \n",
    "    # main loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        ''' Get the Learning Rate '''\n",
    "        current_lr=get_lr(opt)\n",
    "        if(verbose):\n",
    "            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n",
    "        \n",
    "        ''' Train the Model on the Training Set '''\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,check,opt)\n",
    "\n",
    "        ''' Collect loss and metric for training dataset ''' \n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        \n",
    "        ''' Evaluate model on validation dataset '''\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl,check)\n",
    "        \n",
    "        # store best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # store weights into a local file\n",
    "            torch.save(model.state_dict(), weight_path)\n",
    "            if(verbose):\n",
    "                print(\"Copied best model weights!\")\n",
    "        \n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        \n",
    "        # learning rate schedule\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            if(verbose):\n",
    "                print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts) \n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n",
    "            print(\"-\"*10) \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e01833",
   "metadata": {
    "papermill": {
     "duration": 396.031886,
     "end_time": "2022-07-08T12:23:23.633421",
     "exception": false,
     "start_time": "2022-07-08T12:16:47.601535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params_train={\n",
    " \"train\": train_dl,\"val\": val_dl,\n",
    " \"epochs\": 50,\n",
    " \"optimiser\": optim.Adam(cnn_model.parameters(),\n",
    "                         lr=3e-4),\n",
    " \"lr_change\": ReduceLROnPlateau(opt,\n",
    "                                mode='min',\n",
    "                                factor=0.5,\n",
    "                                patience=20,\n",
    "                                verbose=0),\n",
    " \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n",
    " \"weight_path\": \"weights.pt\",\n",
    " \"check\": False, \n",
    "}\n",
    "\n",
    "''' Actual Train / Evaluation of CNN Model '''\n",
    "# train and validate the model\n",
    "cnn_model,loss_hist,metric_hist=train_val(cnn_model,params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e386b",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.131865,
     "end_time": "2022-07-08T12:23:23.810766",
     "exception": false,
     "start_time": "2022-07-08T12:23:23.678901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train-Validation Progress\n",
    "epochs=params_train[\"epochs\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,subplot_titles=['lost_hist','metric_hist'])\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=loss_hist[\"train\"],name='loss_hist[\"train\"]'),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=loss_hist[\"val\"],name='loss_hist[\"val\"]'),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=metric_hist[\"train\"],name='metric_hist[\"train\"]'),row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[*range(1,epochs+1)], y=metric_hist[\"val\"],name='metric_hist[\"val\"]'),row=1, col=2)\n",
    "fig.update_layout(template='plotly_white')\n",
    "fig.update_layout(margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0},height=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedb111",
   "metadata": {},
   "source": [
    "## Obtain label for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=imgpath+train_img_names_raw[0]\n",
    "image = plt.imread(image_path)\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab846",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([transforms.ToTensor(),transforms.Resize((46,46))])\n",
    "image_tensor = my_transforms(image).unsqueeze(0) \n",
    "output=cnn_model(image_tensor)\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 455.740821,
   "end_time": "2022-07-08T12:23:26.435019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-08T12:15:50.694198",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
